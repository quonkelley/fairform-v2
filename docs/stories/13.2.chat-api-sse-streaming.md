# Story 13.2: Chat API with SSE Streaming

## Status
Done

## Story
**As a** FairForm user,
**I want** to send messages to the AI Copilot and receive streaming responses in real-time,
**so that** I get immediate feedback and a natural conversational experience.

## Acceptance Criteria

1. API endpoint `/api/ai/copilot/chat` accepts POST requests with message, sessionId, caseId, and demo flag
2. Endpoint validates request body using Zod schema
3. Endpoint requires authentication (except demo mode with token)
4. Response streams via Server-Sent Events (SSE) with proper content-type headers
5. SSE stream emits: meta event (session/message IDs), delta events (content chunks), done event (completion stats)
6. JSON fallback response available if SSE not supported
7. OpenAI API integration uses gpt-4o-mini model with proper error handling and retries
8. Content moderation applied pre-call (user input) and post-call (assistant output)
9. Messages persisted to Firestore via aiSessionsRepo after successful completion
10. Error responses include appropriate HTTP status codes and error messages

## Tasks / Subtasks

- [x] Task 1: Create request/response schemas (AC: 2)
  - [x] Create `lib/ai/schemas.ts` (or extend existing)
  - [x] Define ChatRequestSchema with Zod: message (string, 1-2000 chars), sessionId (optional), caseId (optional), demo (optional boolean)
  - [x] Define ChatResponseSchema for JSON fallback
  - [x] Export TypeScript types from schemas

- [x] Task 2: Create API route handler (AC: 1, 3)
  - [x] Create `app/api/ai/copilot/chat/route.ts`
  - [x] Implement POST handler function
  - [x] Add authentication check using requireAuth (skip for demo mode)
  - [x] Parse and validate request body with ChatRequestSchema
  - [x] Return 400 for invalid requests with validation errors

- [x] Task 3: Implement session management (AC: 9)
  - [x] Check if sessionId provided, if not create new session via aiSessionsRepo
  - [x] If caseId provided, update session context
  - [x] Store user message to Firestore immediately
  - [x] Generate unique messageId for assistant response

- [x] Task 4: Set up OpenAI API integration (AC: 7)
  - [x] Import OpenAI SDK (openai package)
  - [x] Configure with OPENAI_API_KEY from env
  - [x] Use model: 'gpt-4o-mini', temperature: 0.2, max_tokens: 1000
  - [x] Implement retry logic with exponential backoff (max 2 retries)
  - [x] Handle rate limits (429), quota errors, and timeouts
  - [x] Create AIServiceError class for categorized errors

- [x] Task 5: Implement content moderation (AC: 8)
  - [x] Create `lib/ai/moderate.ts` if not exists
  - [x] Implement moderateUserInput function using OpenAI moderation API
  - [x] Check user message before sending to chat completion
  - [x] Return 400 with error if content blocked
  - [x] Implement moderateAssistantOutput function
  - [x] Block and log if assistant output flagged

- [x] Task 6: Implement SSE streaming response (AC: 4, 5)
  - [x] Set response headers: 'Content-Type': 'text/event-stream', 'Cache-Control': 'no-cache', 'Connection': 'keep-alive'
  - [x] Create ReadableStream for SSE
  - [x] Emit 'meta' event with sessionId, messageId, model, startedAt
  - [x] Stream OpenAI response chunks as 'delta' events
  - [x] Accumulate full response text while streaming
  - [x] Emit 'done' event with tokensIn, tokensOut, latencyMs
  - [x] Handle stream errors gracefully

- [x] Task 7: Implement JSON fallback (AC: 6)
  - [x] Detect if client doesn't support SSE (check Accept header)
  - [x] Call OpenAI without streaming
  - [x] Return complete response as JSON
  - [x] Include same metadata as SSE done event

- [x] Task 8: Persist assistant message (AC: 9)
  - [x] After streaming completes, save full assistant message to Firestore
  - [x] Include meta data: tokensIn, tokensOut, latencyMs, model
  - [x] Update session lastMessageAt timestamp
  - [x] Handle persistence errors without failing the response

- [x] Task 9: Implement comprehensive error handling (AC: 10)
  - [x] 400: Invalid request, validation errors, blocked content
  - [x] 401: Authentication required
  - [x] 429: Rate limit exceeded (include retry-after)
  - [x] 500: Internal server error
  - [x] 502: OpenAI service unavailable
  - [x] Log all errors with context for debugging

- [x] Task 10: Add integration tests (AC: all)
  - [x] Test successful chat with SSE streaming
  - [x] Test JSON fallback mode
  - [x] Test session creation when sessionId not provided
  - [x] Test authentication requirement
  - [x] Test content moderation blocking
  - [x] Test OpenAI error handling
  - [x] Test message persistence to Firestore
  - [x] Mock OpenAI API and Firestore

## Dev Notes

### Architecture Context
[Source: docs/epic-13-unified-architecture-specification.md]

This story implements the core API endpoint for AI Copilot chat functionality. The endpoint supports **Server-Sent Events (SSE)** for real-time streaming responses with a JSON fallback for compatibility.

**Key Design Decisions:**
- SSE streaming for better UX (lower time-to-first-byte)
- JSON fallback for compatibility
- Pre/post content moderation for safety
- Retry logic with exponential backoff for reliability
- Separate demo mode for sandbox testing

### API Contract

**Request:**
```typescript
POST /api/ai/copilot/chat
Content-Type: application/json

{
  message: string;        // 1-2000 chars
  sessionId?: string;     // Create new if omitted
  caseId?: string;        // Optional case context
  demo?: boolean;         // Enable sandbox mode
}
```

**SSE Response:**
```
Content-Type: text/event-stream

event: meta
data: {"sessionId":"sess_123","messageId":"msg_abc","model":"gpt-4o-mini","startedAt":1704067200000}

event: delta
data: {"chunk":"Hello, I'm your FairForm Copilot..."}

event: delta
data: {"chunk":" Let's review your case."}

event: done
data: {"tokensIn":453,"tokensOut":212,"latencyMs":1420}
```

**JSON Fallback:**
```json
{
  "sessionId": "sess_123",
  "messageId": "msg_abc",
  "reply": "Hello, I'm your FairForm Copilot... Let's review your case.",
  "meta": {
    "tokensIn": 453,
    "tokensOut": 212,
    "latencyMs": 1420
  }
}
```

### Authentication
[Source: lib/auth/server-auth.ts]

Use existing `requireAuth` function:
```typescript
import { requireAuth } from '@/lib/auth/server-auth';

export async function POST(request: NextRequest) {
  // Skip auth for demo mode (check demo token)
  if (!isDemoRequest(request)) {
    const user = await requireAuth(request);
  }
  // ... rest of handler
}
```

### OpenAI Integration

**Model Configuration:**
- Model: `gpt-4o-mini`
- Temperature: `0.2` (consistent, focused responses)
- Max Tokens: `1000`
- Timeout: 30 seconds

**Error Handling:**
```typescript
try {
  const response = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [...],
    temperature: 0.2,
    max_tokens: 1000,
    stream: true  // For SSE
  });
} catch (error) {
  if (error.status === 429) {
    // Rate limit - retry with backoff
  } else if (error.status >= 500) {
    // OpenAI service error - retry once
  } else {
    // Other errors - fail gracefully
  }
}
```

### Content Moderation

**Pre-call (User Input):**
```typescript
const moderation = await openai.moderations.create({
  input: userMessage
});

if (moderation.results[0].flagged) {
  return NextResponse.json(
    { error: 'Content blocked', categories: moderation.results[0].categories },
    { status: 400 }
  );
}
```

**Post-call (Assistant Output):**
```typescript
const outputModeration = await openai.moderations.create({
  input: assistantMessage
});

if (outputModeration.results[0].flagged) {
  // Don't store blocked output
  // Log incident
  // Return safe fallback message
}
```

### SSE Implementation in Next.js

```typescript
export async function POST(request: NextRequest) {
  // ... validation and setup ...
  
  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      // Send meta event
      controller.enqueue(encoder.encode(`event: meta\ndata: ${JSON.stringify(meta)}\n\n`));
      
      // Stream OpenAI response
      for await (const chunk of openaiStream) {
        const content = chunk.choices[0]?.delta?.content;
        if (content) {
          controller.enqueue(encoder.encode(`event: delta\ndata: ${JSON.stringify({chunk: content})}\n\n`));
        }
      }
      
      // Send done event
      controller.enqueue(encoder.encode(`event: done\ndata: ${JSON.stringify(stats)}\n\n`));
      controller.close();
    }
  });
  
  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    }
  });
}
```

### Source Tree
```
app/api/ai/copilot/
└── chat/
    └── route.ts           # NEW: SSE streaming endpoint

lib/
├── ai/
│   ├── schemas.ts         # NEW: Request/response schemas
│   ├── moderate.ts        # NEW: Content moderation
│   └── types.ts           # EXISTING: From Story 13.1
├── auth/
│   └── server-auth.ts     # EXISTING: Use requireAuth
└── db/
    └── aiSessionsRepo.ts  # EXISTING: From Story 13.1
```

### Environment Variables
```bash
OPENAI_API_KEY=sk-...
AI_MODEL=gpt-4o-mini
AI_TEMPERATURE=0.2
AI_MAX_TOKENS=1000
DEMO_TOKEN=demo-secret-token
```

### Testing

**Test Location:** `app/api/ai/copilot/chat/route.test.ts`

**Test Strategy:**
- Mock OpenAI SDK responses
- Mock Firestore operations
- Test SSE event parsing
- Test error scenarios
- Test authentication flows

**Example Test:**
```typescript
describe('POST /api/ai/copilot/chat', () => {
  it('streams SSE response successfully', async () => {
    const response = await fetch('/api/ai/copilot/chat', {
      method: 'POST',
      body: JSON.stringify({ message: 'Hello' })
    });
    
    expect(response.headers.get('content-type')).toContain('text/event-stream');
    // Parse SSE events and verify
  });
  
  it('blocks inappropriate content', async () => {
    // Mock moderation API to flag content
    const response = await fetch('/api/ai/copilot/chat', {
      method: 'POST',
      body: JSON.stringify({ message: 'inappropriate content' })
    });
    
    expect(response.status).toBe(400);
  });
});
```

### Performance Targets
- Time to first byte (TTFB): < 500ms
- Total response time: < 3s
- Token usage: Monitor and log for cost tracking

### Dependencies
- openai (npm package) - Install if not present
- aiSessionsRepo from Story 13.1
- requireAuth from existing auth setup

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-14 | 1.1 | Addressed QA findings: post-stream moderation, streaming retry integration, session case syncing, and test hardening | James (Full Stack Developer) |
| 2025-01-13 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
OpenAI GPT-5 Codex (Codex CLI)

### Debug Log References
- Refactored SSE handler to reuse shared retry helper and emit structured error events
- Added post-stream moderation guard with safe fallback persistence
- Synced existing session case associations through new repository helper
- Reworked integration test harness with hoisted mocks and SSE moderation coverage

### Completion Notes List
- **API Route**: Hardened `/app/api/ai/copilot/chat/route.ts` with shared retry helper, structured SSE error events, and improved token accounting
- **Session Management**: Added `updateSessionCase` flow so existing sessions adopt newly supplied case IDs
- **Safety Pipeline**: Added post-stream moderation fallback with `moderation_blocked` SSE event and guarded persistence
- **Types & Metadata**: Extended `MessageMeta` to capture moderation context for analytics consumers
- **Testing**: Reworked `tests/api/ai/copilot/chat.test.ts` with hoisted OpenAI mocks and coverage for streaming moderation/error paths

### File List
- **Modified**: `app/api/ai/copilot/chat/route.ts` – Streaming retry integration, moderation fallback, SSE error events
- **Modified**: `lib/db/aiSessionsRepo.ts` – Added `updateSessionCase` helper for session context alignment
- **Modified**: `lib/ai/types.ts` – Extended `MessageMeta` with moderation metadata fields
- **Modified**: `tests/api/ai/copilot/chat.test.ts` – Hoisted OpenAI mocks and added streaming moderation coverage

## QA Results
- **Decision: PASS** – All blocking issues addressed in v1.1. Type safety issues resolved.
- **Previously Blocking Issues (Fixed in v1.1)**
  - ✅ SSE responses now run post-call moderation with safe fallback persistence
  - ✅ SSE implementation integrated with retry helper and error classification
  - ✅ Session case associations properly synced through `updateSessionCase` helper
  - ✅ Token counting improved with proper OpenAI usage tracking
- **Test Coverage**
  - ✅ All integration tests passing with hoisted mocks
  - ✅ Streaming moderation coverage added
  - ✅ Error path testing comprehensive
- **Type Safety (Fixed 2025-10-14)**
  - ✅ All TypeScript errors resolved (User type updated with id, timeZone, and AI preferences)
  - ✅ Test mocks updated to match actual types
  - ✅ Zero TypeScript compilation errors
