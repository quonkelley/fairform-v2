# Story 13.19: Performance Testing

## Status
Draft (low priority)

## Story
**As a** FairForm QA engineer,
**I want** comprehensive performance testing for the AI Copilot system,
**so that** we can validate latency targets, identify bottlenecks, and ensure the system performs well under load.

## Acceptance Criteria

1. Chat API latency is < 3s at p95 (95th percentile)
2. Context building completes in < 100ms
3. Message pagination responds in < 200ms
4. Session creation completes in < 150ms
5. SSE streaming begins within 500ms of request
6. System handles 100 concurrent chat requests without degradation
7. Performance tests run in CI/CD pipeline
8. Performance regression alerts are configured

## Tasks / Subtasks

- [ ] Task 1: Set up performance testing infrastructure (AC: 7)
  - [ ] Install performance testing tools (k6, Artillery, or similar)
  - [ ] Create performance test configuration
  - [ ] Set up test data fixtures
  - [ ] Configure CI/CD integration
  - [ ] Set up performance reporting

- [ ] Task 2: Create chat API latency tests (AC: 1)
  - [ ] Test single chat request latency
  - [ ] Test chat request with context
  - [ ] Test SSE streaming latency
  - [ ] Measure p50, p95, p99 latencies
  - [ ] Test with different message sizes

- [ ] Task 3: Create context building tests (AC: 2)
  - [ ] Test context building with no case
  - [ ] Test context building with case data
  - [ ] Test context building with large case history
  - [ ] Test fingerprint generation performance
  - [ ] Measure memory usage

- [ ] Task 4: Create pagination tests (AC: 3)
  - [ ] Test message list with 20 messages
  - [ ] Test message list with 100+ messages
  - [ ] Test pagination cursor performance
  - [ ] Test concurrent pagination requests
  - [ ] Measure query execution time

- [ ] Task 5: Create session management tests (AC: 4)
  - [ ] Test session creation performance
  - [ ] Test session retrieval performance
  - [ ] Test concurrent session operations
  - [ ] Test session update performance
  - [ ] Measure Firestore operation latency

- [ ] Task 6: Create load tests (AC: 6)
  - [ ] Test 10 concurrent users
  - [ ] Test 50 concurrent users
  - [ ] Test 100 concurrent users
  - [ ] Test sustained load over 10 minutes
  - [ ] Monitor system resource usage

- [ ] Task 7: Create SSE streaming tests (AC: 5)
  - [ ] Test SSE connection establishment time
  - [ ] Test first chunk delivery time
  - [ ] Test streaming throughput
  - [ ] Test connection stability
  - [ ] Test concurrent SSE streams

- [ ] Task 8: Set up monitoring and alerting (AC: 8)
  - [ ] Configure performance metrics collection
  - [ ] Set up latency threshold alerts
  - [ ] Create performance dashboards
  - [ ] Integrate with CI/CD
  - [ ] Document performance baselines

## Dev Notes

### Architecture Context
[Source: docs/epic-13-unified-architecture-specification.md, Section 9.3, 10]

Performance testing ensures the AI Copilot meets **< 3s chat latency targets** and performs well under load. The architecture includes multiple optimization points that need validation:

**Key Performance Targets:**
- Chat latency: < 3s (p95)
- Context building: < 100ms
- Message pagination: < 200ms
- Session creation: < 150ms
- SSE streaming start: < 500ms

### Performance Testing Tools

**Tool Selection:**
We'll use **k6** for performance testing due to its:
- JavaScript/TypeScript support
- Excellent HTTP/2 and SSE support
- Built-in metrics and thresholds
- CI/CD integration
- Open source with good documentation

**Installation:**
```bash
npm install --save-dev k6
```

### Test Scripts

**Chat API Latency Test:**
```typescript
// tests/performance/chat-latency.test.ts

import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

// Custom metrics
const chatLatency = new Trend('chat_latency');
const chatErrorRate = new Rate('chat_errors');

// Test configuration
export const options = {
  stages: [
    { duration: '30s', target: 10 },  // Ramp up to 10 users
    { duration: '1m', target: 10 },   // Stay at 10 users
    { duration: '30s', target: 50 },  // Ramp up to 50 users
    { duration: '1m', target: 50 },   // Stay at 50 users
    { duration: '30s', target: 0 },   // Ramp down
  ],
  thresholds: {
    'chat_latency': ['p(95)<3000'], // p95 < 3s
    'chat_errors': ['rate<0.05'],   // Error rate < 5%
    'http_req_duration': ['p(95)<3000'],
  },
};

const BASE_URL = __ENV.API_URL || 'http://localhost:3000';
const AUTH_TOKEN = __ENV.TEST_AUTH_TOKEN;

export default function() {
  const payload = JSON.stringify({
    sessionId: `test-session-${__VU}-${__ITER}`,
    message: 'What are the steps to file a small claims case in California?',
    demo: true
  });

  const params = {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${AUTH_TOKEN}`,
    },
    timeout: '10s',
  };

  const startTime = Date.now();
  const response = http.post(`${BASE_URL}/api/ai/copilot/chat`, payload, params);
  const duration = Date.now() - startTime;

  // Check response
  const success = check(response, {
    'status is 200': (r) => r.status === 200,
    'has reply': (r) => JSON.parse(r.body).reply !== undefined,
    'latency < 3s': () => duration < 3000,
  });

  // Record metrics
  chatLatency.add(duration);
  chatErrorRate.add(!success);

  // Think time between requests
  sleep(Math.random() * 2 + 1); // 1-3 seconds
}
```

**Context Building Performance:**
```typescript
// tests/performance/context-building.test.ts

import { check } from 'k6';
import { Trend } from 'k6/metrics';
import { buildPromptContext } from '../../lib/ai/contextBuilder';

const contextBuildTime = new Trend('context_build_time');

export const options = {
  vus: 1,
  iterations: 100,
  thresholds: {
    'context_build_time': ['p(95)<100'], // p95 < 100ms
  },
};

export default async function() {
  const userId = 'test-user-123';
  const caseId = 'test-case-456';

  const startTime = Date.now();

  // Build context
  const context = await buildPromptContext(userId, caseId);

  const duration = Date.now() - startTime;

  check(context, {
    'has user': (c) => c.user !== undefined,
    'has case': (c) => c.case !== undefined,
    'has fingerprint': (c) => c.fingerprint !== undefined,
    'under 100ms': () => duration < 100,
  });

  contextBuildTime.add(duration);
}
```

**Message Pagination Test:**
```typescript
// tests/performance/pagination.test.ts

import http from 'k6/http';
import { check } from 'k6';
import { Trend } from 'k6/metrics';

const paginationLatency = new Trend('pagination_latency');

export const options = {
  vus: 20,
  duration: '2m',
  thresholds: {
    'pagination_latency': ['p(95)<200'], // p95 < 200ms
    'http_req_duration': ['p(95)<200'],
  },
};

const BASE_URL = __ENV.API_URL || 'http://localhost:3000';
const AUTH_TOKEN = __ENV.TEST_AUTH_TOKEN;

export default function() {
  const sessionId = 'test-session-with-messages';

  const params = {
    headers: {
      'Authorization': `Bearer ${AUTH_TOKEN}`,
    },
  };

  const startTime = Date.now();
  const response = http.get(
    `${BASE_URL}/api/ai/copilot/messages?sessionId=${sessionId}&limit=20`,
    params
  );
  const duration = Date.now() - startTime;

  check(response, {
    'status is 200': (r) => r.status === 200,
    'has items': (r) => JSON.parse(r.body).items !== undefined,
    'under 200ms': () => duration < 200,
  });

  paginationLatency.add(duration);
}
```

**SSE Streaming Test:**
```typescript
// tests/performance/sse-streaming.test.ts

import http from 'k6/http';
import { check } from 'k6';
import { Trend } from 'k6/metrics';

const streamStartTime = new Trend('stream_start_time');
const firstChunkTime = new Trend('first_chunk_time');

export const options = {
  vus: 10,
  duration: '1m',
  thresholds: {
    'stream_start_time': ['p(95)<500'],  // Stream starts < 500ms
    'first_chunk_time': ['p(95)<1000'],  // First chunk < 1s
  },
};

const BASE_URL = __ENV.API_URL || 'http://localhost:3000';
const AUTH_TOKEN = __ENV.TEST_AUTH_TOKEN;

export default function() {
  const payload = JSON.stringify({
    sessionId: `stream-test-${__VU}-${__ITER}`,
    message: 'Tell me about small claims court.',
    demo: true
  });

  const params = {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${AUTH_TOKEN}`,
      'Accept': 'text/event-stream',
    },
    timeout: '10s',
  };

  const requestStart = Date.now();
  const response = http.post(`${BASE_URL}/api/ai/copilot/chat`, payload, params);
  const streamStart = Date.now() - requestStart;

  // Parse SSE response to find first delta
  const events = response.body.split('\n\n');
  let firstChunk = 0;

  for (let i = 0; i < events.length; i++) {
    if (events[i].includes('event: delta')) {
      firstChunk = Date.now() - requestStart;
      break;
    }
  }

  check(response, {
    'is SSE': (r) => r.headers['Content-Type']?.includes('text/event-stream'),
    'stream starts quickly': () => streamStart < 500,
    'first chunk quickly': () => firstChunk > 0 && firstChunk < 1000,
  });

  streamStartTime.add(streamStart);
  if (firstChunk > 0) {
    firstChunkTime.add(firstChunk);
  }
}
```

**Load Test:**
```typescript
// tests/performance/load-test.test.ts

import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Counter } from 'k6/metrics';

const errorRate = new Rate('errors');
const requestCount = new Counter('requests');

export const options = {
  stages: [
    { duration: '2m', target: 10 },   // Warm up
    { duration: '5m', target: 50 },   // Ramp up
    { duration: '10m', target: 100 }, // Peak load
    { duration: '5m', target: 50 },   // Ramp down
    { duration: '2m', target: 0 },    // Cool down
  ],
  thresholds: {
    'errors': ['rate<0.1'],           // Error rate < 10%
    'http_req_duration': ['p(95)<5000'], // Relaxed under load
    'http_req_failed': ['rate<0.1'],
  },
};

const BASE_URL = __ENV.API_URL || 'http://localhost:3000';
const AUTH_TOKEN = __ENV.TEST_AUTH_TOKEN;

const SAMPLE_MESSAGES = [
  'What is a small claims case?',
  'How do I respond to an eviction notice?',
  'What documents do I need for court?',
  'Can you explain the filing process?',
  'What are the deadlines for my case?',
];

export default function() {
  requestCount.add(1);

  // Random message
  const message = SAMPLE_MESSAGES[Math.floor(Math.random() * SAMPLE_MESSAGES.length)];

  const payload = JSON.stringify({
    sessionId: `load-test-${__VU}`,
    message,
    demo: true
  });

  const params = {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${AUTH_TOKEN}`,
    },
  };

  const response = http.post(`${BASE_URL}/api/ai/copilot/chat`, payload, params);

  const success = check(response, {
    'status is 200': (r) => r.status === 200,
    'has reply': (r) => {
      try {
        return JSON.parse(r.body).reply !== undefined;
      } catch {
        return false;
      }
    },
  });

  errorRate.add(!success);

  // Realistic think time
  sleep(Math.random() * 5 + 2); // 2-7 seconds between messages
}
```

### CI/CD Integration

**GitHub Actions Workflow:**
```yaml
# .github/workflows/performance-tests.yml

name: Performance Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * *' # Daily at 2 AM

jobs:
  performance-tests:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build

      - name: Start test server
        run: |
          npm run start &
          sleep 10
        env:
          NODE_ENV: test
          NEXT_PUBLIC_DEMO_MODE: true

      - name: Install k6
        run: |
          curl -L https://github.com/grafana/k6/releases/download/v0.47.0/k6-v0.47.0-linux-amd64.tar.gz | tar xvz
          sudo mv k6-v0.47.0-linux-amd64/k6 /usr/local/bin/

      - name: Run performance tests
        run: |
          k6 run tests/performance/chat-latency.test.ts --out json=results.json
        env:
          API_URL: http://localhost:3000
          TEST_AUTH_TOKEN: ${{ secrets.TEST_AUTH_TOKEN }}

      - name: Check thresholds
        run: |
          if grep -q '"thresholds":{".*":false}' results.json; then
            echo "Performance thresholds failed!"
            exit 1
          fi

      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: results.json

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('results.json', 'utf8'));

            const comment = `## Performance Test Results

            - Chat Latency (p95): ${results.metrics.chat_latency['p(95)']}ms
            - Error Rate: ${(results.metrics.chat_errors.rate * 100).toFixed(2)}%
            - Requests: ${results.metrics.http_reqs.count}

            ${results.thresholds_passed ? '✅ All thresholds passed' : '❌ Some thresholds failed'}`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
```

### Performance Monitoring

**Metrics Collection:**
```typescript
// lib/monitoring/metrics.ts

export class PerformanceMonitor {
  private metrics: Map<string, number[]> = new Map();

  recordLatency(operation: string, duration: number): void {
    if (!this.metrics.has(operation)) {
      this.metrics.set(operation, []);
    }
    this.metrics.get(operation)!.push(duration);
  }

  getStats(operation: string): {
    p50: number;
    p95: number;
    p99: number;
    avg: number;
    count: number;
  } | null {
    const values = this.metrics.get(operation);
    if (!values || values.length === 0) return null;

    const sorted = values.slice().sort((a, b) => a - b);
    const count = sorted.length;

    return {
      p50: sorted[Math.floor(count * 0.5)],
      p95: sorted[Math.floor(count * 0.95)],
      p99: sorted[Math.floor(count * 0.99)],
      avg: sorted.reduce((a, b) => a + b, 0) / count,
      count
    };
  }

  reset(): void {
    this.metrics.clear();
  }
}

export const performanceMonitor = new PerformanceMonitor();
```

### Source Tree
```
tests/performance/
├── chat-latency.test.ts       # NEW: Chat API latency tests
├── context-building.test.ts   # NEW: Context building tests
├── pagination.test.ts         # NEW: Pagination tests
├── sse-streaming.test.ts      # NEW: SSE streaming tests
└── load-test.test.ts          # NEW: Load testing

lib/monitoring/
└── metrics.ts                 # NEW: Performance monitoring

.github/workflows/
└── performance-tests.yml      # NEW: CI/CD integration

scripts/
└── run-performance-tests.sh   # NEW: Test runner script
```

### Running Tests

**Local Testing:**
```bash
# Install k6
brew install k6  # macOS
# or download from https://k6.io/docs/getting-started/installation/

# Run specific test
k6 run tests/performance/chat-latency.test.ts

# Run with custom duration
k6 run tests/performance/load-test.test.ts --duration=5m --vus=50

# Run all tests
npm run test:performance
```

**package.json Scripts:**
```json
{
  "scripts": {
    "test:performance": "k6 run tests/performance/*.test.ts",
    "test:performance:ci": "k6 run tests/performance/chat-latency.test.ts --out json=results.json",
    "test:performance:report": "k6 run tests/performance/*.test.ts --out html=report.html"
  }
}
```

### Performance Baselines

**Target Metrics:**
```typescript
export const PERFORMANCE_TARGETS = {
  chatLatency: {
    p50: 1500,  // 1.5s
    p95: 3000,  // 3s
    p99: 5000   // 5s
  },
  contextBuilding: {
    p95: 100    // 100ms
  },
  messagePagination: {
    p95: 200    // 200ms
  },
  sessionCreation: {
    p95: 150    // 150ms
  },
  sseStreamStart: {
    p95: 500    // 500ms
  },
  errorRate: {
    max: 0.05   // 5%
  }
};
```

### Dependencies
- k6 for performance testing
- GitHub Actions for CI/CD
- Test fixtures and sample data
- Monitoring infrastructure

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-13 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent*

### Debug Log References
_To be populated by dev agent*

### Completion Notes List
_To be populated by dev agent*

### File List
_To be populated by dev agent*

## QA Results
_To be populated by QA agent*
