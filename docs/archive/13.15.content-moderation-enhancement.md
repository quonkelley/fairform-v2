# Story 13.15: Content Moderation Enhancement

## Status
Draft (low priority)

## Story
**As a** FairForm compliance officer,
**I want** comprehensive content moderation for both user input and AI responses,
**so that** inappropriate, harmful, or non-compliant content is detected and blocked before being stored or displayed.

## Acceptance Criteria

1. Pre-call moderation validates user input before sending to OpenAI API
2. Post-call moderation validates AI responses before storage and display
3. Moderation flags detect prohibited categories (hate, violence, self-harm, sexual, etc.)
4. Blocked content triggers appropriate user-facing messages
5. All moderation events are logged for audit and compliance
6. Moderation service handles API failures gracefully
7. Moderation bypass available for testing/development (with logging)
8. Unit tests cover moderation logic and all flag categories

## Tasks / Subtasks

- [ ] Task 1: Create moderation service infrastructure (AC: 1, 2)
  - [ ] Create `lib/ai/moderation.ts`
  - [ ] Implement OpenAI Moderation API integration
  - [ ] Add pre-call moderation function
  - [ ] Add post-call moderation function
  - [ ] Handle API errors and timeouts

- [ ] Task 2: Implement content classification (AC: 3)
  - [ ] Define prohibited content categories
  - [ ] Integrate OpenAI moderation endpoint
  - [ ] Parse and structure moderation results
  - [ ] Add category-specific thresholds
  - [ ] Support custom moderation rules

- [ ] Task 3: Add moderation response handling (AC: 4)
  - [ ] Create user-friendly block messages
  - [ ] Add category-specific error messages
  - [ ] Implement fallback responses
  - [ ] Handle partial blocks (redaction)
  - [ ] Add appeal/retry mechanisms

- [ ] Task 4: Implement moderation logging (AC: 5)
  - [ ] Create moderation event schema
  - [ ] Log all moderation checks (pass/fail)
  - [ ] Store blocked content securely
  - [ ] Add audit trail functionality
  - [ ] Support compliance reporting

- [ ] Task 5: Add graceful degradation (AC: 6)
  - [ ] Handle moderation API failures
  - [ ] Implement retry logic with exponential backoff
  - [ ] Add circuit breaker pattern
  - [ ] Provide safe fallback behavior
  - [ ] Monitor moderation service health

- [ ] Task 6: Create testing infrastructure (AC: 7, 8)
  - [ ] Add moderation bypass flag
  - [ ] Implement test mode logging
  - [ ] Create test fixtures for all categories
  - [ ] Add unit tests for moderation logic
  - [ ] Test all flag categories
  - [ ] Test error handling

- [ ] Task 7: Integrate with chat API (AC: 1, 2)
  - [ ] Add pre-call moderation to chat endpoint
  - [ ] Add post-call moderation before message storage
  - [ ] Handle moderation failures gracefully
  - [ ] Return appropriate error responses
  - [ ] Update API documentation

- [ ] Task 8: Add monitoring and metrics (AC: 5)
  - [ ] Track moderation API calls
  - [ ] Monitor flag rates by category
  - [ ] Alert on moderation failures
  - [ ] Track false positive patterns
  - [ ] Generate compliance reports

## Dev Notes

### Architecture Context
[Source: docs/epic-13-unified-architecture-specification.md, Section 4.4]

Content moderation is **essential for compliance and safety** - it prevents inappropriate content from entering the system and ensures AI responses meet ethical standards. The system uses **OpenAI's Moderation API** with comprehensive logging and graceful fallback behavior.

**Key Design Decisions:**
- Pre-call moderation for user input
- Post-call moderation for AI responses
- Comprehensive logging for all moderation events
- Graceful degradation when moderation API unavailable
- Category-specific user messages

### Moderation Service Architecture

**Core Interface:**
```typescript
// lib/ai/moderation.ts

export interface ModerationResult {
  flagged: boolean;
  categories: ModerationCategories;
  categoryScores: ModerationCategoryScores;
  blocked: boolean;
  reason?: string;
}

export interface ModerationCategories {
  hate: boolean;
  'hate/threatening': boolean;
  'self-harm': boolean;
  sexual: boolean;
  'sexual/minors': boolean;
  violence: boolean;
  'violence/graphic': boolean;
}

export interface ModerationCategoryScores {
  hate: number;
  'hate/threatening': number;
  'self-harm': number;
  sexual: number;
  'sexual/minors': number;
  violence: number;
  'violence/graphic': number;
}

export interface ModerationEvent {
  id: string;
  timestamp: number;
  userId: string;
  sessionId: string;
  contentType: 'user_input' | 'ai_response';
  content: string; // Securely stored
  result: ModerationResult;
  action: 'allowed' | 'blocked';
}

export class ModerationError extends Error {
  constructor(
    message: string,
    public code: 'API_ERROR' | 'TIMEOUT' | 'RATE_LIMIT' | 'BLOCKED',
    public category?: string
  ) {
    super(message);
    this.name = 'ModerationError';
  }
}
```

### Moderation Service Implementation

**Main Service:**
```typescript
import OpenAI from 'openai';
import { createHash } from 'crypto';

export class ContentModerationService {
  private openai: OpenAI;
  private bypassEnabled: boolean;

  constructor(apiKey: string, bypassEnabled: boolean = false) {
    this.openai = new OpenAI({ apiKey });
    this.bypassEnabled = bypassEnabled;
  }

  /**
   * Pre-call moderation: Validate user input before sending to AI
   */
  async moderateUserInput(
    text: string,
    userId: string,
    sessionId: string
  ): Promise<ModerationResult> {
    // Bypass in test mode (with logging)
    if (this.bypassEnabled) {
      console.warn('[MODERATION] Bypass enabled - skipping user input moderation');
      return this.createBypassResult();
    }

    try {
      const response = await this.openai.moderations.create({
        input: text
      });

      const result = response.results[0];

      const moderationResult: ModerationResult = {
        flagged: result.flagged,
        categories: result.categories,
        categoryScores: result.category_scores,
        blocked: result.flagged,
        reason: result.flagged ? this.determineBlockReason(result.categories) : undefined
      };

      // Log moderation event
      await this.logModerationEvent({
        id: this.generateEventId(),
        timestamp: Date.now(),
        userId,
        sessionId,
        contentType: 'user_input',
        content: text,
        result: moderationResult,
        action: moderationResult.blocked ? 'blocked' : 'allowed'
      });

      return moderationResult;

    } catch (error: any) {
      console.error('[MODERATION] User input moderation failed:', error);

      // Fail closed for user input - block on error
      if (error.status === 429) {
        throw new ModerationError('Moderation rate limit exceeded', 'RATE_LIMIT');
      }

      throw new ModerationError('Moderation service unavailable', 'API_ERROR');
    }
  }

  /**
   * Post-call moderation: Validate AI response before storage
   */
  async moderateAIResponse(
    text: string,
    userId: string,
    sessionId: string
  ): Promise<ModerationResult> {
    // Bypass in test mode (with logging)
    if (this.bypassEnabled) {
      console.warn('[MODERATION] Bypass enabled - skipping AI response moderation');
      return this.createBypassResult();
    }

    try {
      const response = await this.openai.moderations.create({
        input: text
      });

      const result = response.results[0];

      const moderationResult: ModerationResult = {
        flagged: result.flagged,
        categories: result.categories,
        categoryScores: result.category_scores,
        blocked: result.flagged,
        reason: result.flagged ? this.determineBlockReason(result.categories) : undefined
      };

      // Log moderation event
      await this.logModerationEvent({
        id: this.generateEventId(),
        timestamp: Date.now(),
        userId,
        sessionId,
        contentType: 'ai_response',
        content: text,
        result: moderationResult,
        action: moderationResult.blocked ? 'blocked' : 'allowed'
      });

      return moderationResult;

    } catch (error: any) {
      console.error('[MODERATION] AI response moderation failed:', error);

      // Fail closed for AI responses - block on error
      if (error.status === 429) {
        throw new ModerationError('Moderation rate limit exceeded', 'RATE_LIMIT');
      }

      throw new ModerationError('Moderation service unavailable', 'API_ERROR');
    }
  }

  /**
   * Determine primary block reason from flagged categories
   */
  private determineBlockReason(categories: ModerationCategories): string {
    const flaggedCategories = Object.entries(categories)
      .filter(([_, flagged]) => flagged)
      .map(([category]) => category);

    if (flaggedCategories.length === 0) {
      return 'Content flagged by moderation system';
    }

    // Priority order for user-facing messages
    const priorityOrder = [
      'sexual/minors',
      'self-harm',
      'violence/graphic',
      'violence',
      'hate/threatening',
      'hate',
      'sexual'
    ];

    for (const category of priorityOrder) {
      if (flaggedCategories.includes(category)) {
        return this.getCategoryMessage(category);
      }
    }

    return 'Content flagged by moderation system';
  }

  /**
   * Get user-friendly message for category
   */
  private getCategoryMessage(category: string): string {
    const messages: Record<string, string> = {
      'sexual/minors': 'Content involves minors in inappropriate context',
      'self-harm': 'Content involves self-harm or suicide',
      'violence/graphic': 'Content contains graphic violence',
      'violence': 'Content contains violent themes',
      'hate/threatening': 'Content contains threatening hate speech',
      'hate': 'Content contains hate speech or discrimination',
      'sexual': 'Content contains inappropriate sexual content'
    };

    return messages[category] || 'Content flagged by moderation system';
  }

  /**
   * Create bypass result for testing
   */
  private createBypassResult(): ModerationResult {
    return {
      flagged: false,
      categories: {
        hate: false,
        'hate/threatening': false,
        'self-harm': false,
        sexual: false,
        'sexual/minors': false,
        violence: false,
        'violence/graphic': false
      },
      categoryScores: {
        hate: 0,
        'hate/threatening': 0,
        'self-harm': 0,
        sexual: 0,
        'sexual/minors': 0,
        violence: 0,
        'violence/graphic': 0
      },
      blocked: false
    };
  }

  /**
   * Log moderation event for audit trail
   */
  private async logModerationEvent(event: ModerationEvent): Promise<void> {
    // Store in Firestore for audit
    const db = getAdminFirestore();

    await db.collection('moderationEvents').doc(event.id).set({
      ...event,
      contentHash: this.hashContent(event.content), // Hash for privacy
      content: undefined // Don't store raw content in logs
    });

    // Also log to console for monitoring
    console.log('[MODERATION]', {
      id: event.id,
      timestamp: new Date(event.timestamp).toISOString(),
      contentType: event.contentType,
      action: event.action,
      flagged: event.result.flagged,
      categories: event.result.flagged ? Object.keys(event.result.categories).filter(k => event.result.categories[k as keyof ModerationCategories]) : []
    });
  }

  /**
   * Generate unique event ID
   */
  private generateEventId(): string {
    return `mod_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Hash content for privacy-preserving audit trail
   */
  private hashContent(content: string): string {
    return createHash('sha256').update(content).digest('hex');
  }
}
```

### Integration with Chat API

**Updated Chat Endpoint:**
```typescript
// app/api/ai/copilot/chat/route.ts

import { ContentModerationService } from '@/lib/ai/moderation';

export async function POST(request: Request) {
  const { sessionId, message, caseId, demo } = await request.json();

  const auth = await requireAuth(request);
  const moderationService = new ContentModerationService(
    process.env.OPENAI_API_KEY!,
    process.env.MODERATION_BYPASS === 'true'
  );

  try {
    // Pre-call moderation: Validate user input
    const userModeration = await moderationService.moderateUserInput(
      message,
      auth.uid,
      sessionId
    );

    if (userModeration.blocked) {
      return NextResponse.json({
        error: 'Content blocked',
        reason: userModeration.reason,
        category: 'moderation'
      }, { status: 400 });
    }

    // Get or create session
    let session = sessionId
      ? await aiSessionsRepo.getSession(sessionId)
      : await aiSessionsRepo.createSession({
          userId: auth.uid,
          caseId,
          demo
        });

    // Store user message
    await aiSessionsRepo.appendMessage(session.id, {
      author: 'user',
      content: message
    });

    // Build context and call OpenAI
    const context = await buildPromptContext(auth.uid, caseId, session.id);
    const aiResponse = await callOpenAI(context, message);

    // Post-call moderation: Validate AI response
    const aiModeration = await moderationService.moderateAIResponse(
      aiResponse,
      auth.uid,
      session.id
    );

    if (aiModeration.blocked) {
      console.error('[MODERATION] AI response blocked:', aiModeration.reason);

      // Store fallback message instead
      const fallbackMessage = "I apologize, but I'm unable to provide that response. Let me try rephrasing.";
      await aiSessionsRepo.appendMessage(session.id, {
        author: 'assistant',
        content: fallbackMessage,
        meta: { blocked: true }
      });

      return NextResponse.json({
        sessionId: session.id,
        messageId: 'fallback',
        reply: fallbackMessage,
        blocked: true
      });
    }

    // Store AI response
    const assistantMessage = await aiSessionsRepo.appendMessage(session.id, {
      author: 'assistant',
      content: aiResponse,
      meta: {
        tokensIn: context.tokenCount,
        tokensOut: aiResponse.length,
        latencyMs: Date.now() - startTime
      }
    });

    return NextResponse.json({
      sessionId: session.id,
      messageId: assistantMessage.id,
      reply: aiResponse
    });

  } catch (error) {
    if (error instanceof ModerationError) {
      return NextResponse.json({
        error: 'Moderation check failed',
        message: error.message,
        code: error.code
      }, { status: error.code === 'BLOCKED' ? 400 : 503 });
    }

    throw error;
  }
}
```

### Error Messages and Fallbacks

**User-Facing Messages:**
```typescript
export const MODERATION_MESSAGES = {
  user_blocked: "I'm unable to process that message as it contains content that violates our community guidelines. Please rephrase your question.",

  ai_blocked: "I apologize, but I'm unable to provide that response. Let me try rephrasing or approaching your question from a different angle.",

  service_unavailable: "Our content moderation system is temporarily unavailable. Please try again in a moment.",

  category_specific: {
    hate: "This content contains language that may be discriminatory or hateful.",
    violence: "This content contains violent themes that we cannot process.",
    'self-harm': "If you're in crisis, please contact a mental health professional or crisis hotline immediately. We're here to help with legal questions.",
    sexual: "This content contains inappropriate material that we cannot process."
  }
};

export function getModerationMessage(result: ModerationResult, contentType: 'user_input' | 'ai_response'): string {
  if (!result.blocked) {
    return '';
  }

  if (contentType === 'ai_response') {
    return MODERATION_MESSAGES.ai_blocked;
  }

  // Check for specific categories
  const flaggedCategory = Object.entries(result.categories)
    .find(([_, flagged]) => flagged)?.[0];

  if (flaggedCategory && flaggedCategory in MODERATION_MESSAGES.category_specific) {
    return MODERATION_MESSAGES.category_specific[flaggedCategory as keyof typeof MODERATION_MESSAGES.category_specific];
  }

  return MODERATION_MESSAGES.user_blocked;
}
```

### Testing

**Test Location:** `lib/ai/moderation.test.ts`

**Test Suite:**
```typescript
describe('ContentModerationService', () => {
  let service: ContentModerationService;

  beforeEach(() => {
    service = new ContentModerationService(process.env.OPENAI_API_KEY!, false);
  });

  it('allows safe user input', async () => {
    const result = await service.moderateUserInput(
      'I need help filing a small claims case',
      'user-123',
      'session-abc'
    );

    expect(result.flagged).toBe(false);
    expect(result.blocked).toBe(false);
  });

  it('blocks hate speech', async () => {
    const result = await service.moderateUserInput(
      'I hate [protected group]',
      'user-123',
      'session-abc'
    );

    expect(result.flagged).toBe(true);
    expect(result.blocked).toBe(true);
    expect(result.categories.hate).toBe(true);
  });

  it('blocks violent content', async () => {
    const result = await service.moderateUserInput(
      'I want to hurt someone',
      'user-123',
      'session-abc'
    );

    expect(result.flagged).toBe(true);
    expect(result.blocked).toBe(true);
    expect(result.categories.violence).toBe(true);
  });

  it('provides category-specific messages', () => {
    const result: ModerationResult = {
      flagged: true,
      blocked: true,
      categories: {
        hate: true,
        'hate/threatening': false,
        'self-harm': false,
        sexual: false,
        'sexual/minors': false,
        violence: false,
        'violence/graphic': false
      },
      categoryScores: { /* ... */ }
    };

    const message = getModerationMessage(result, 'user_input');
    expect(message).toContain('discriminatory');
  });

  it('bypasses moderation in test mode', async () => {
    const testService = new ContentModerationService(process.env.OPENAI_API_KEY!, true);

    const result = await testService.moderateUserInput(
      'test content',
      'user-123',
      'session-abc'
    );

    expect(result.flagged).toBe(false);
    expect(result.blocked).toBe(false);
  });
});
```

### Source Tree
```
lib/ai/
├── moderation.ts          # NEW: Moderation service
├── moderationMessages.ts  # NEW: User-facing messages
└── types.ts               # MODIFIED: Add moderation types

tests/
└── lib/ai/
    └── moderation.test.ts # NEW: Moderation tests

app/api/ai/copilot/
└── chat/route.ts          # MODIFIED: Add moderation checks
```

### Configuration

**Environment Variables:**
```bash
# .env.local
OPENAI_API_KEY=sk-...
MODERATION_BYPASS=false # Set to 'true' in development only
```

### Performance Considerations
- Moderation adds ~200-500ms latency per call
- Consider caching moderation results for identical content
- Implement circuit breaker for moderation API failures
- Monitor moderation API rate limits

### Dependencies
- OpenAI SDK for moderation endpoint
- Firebase Admin SDK for audit logging
- Existing aiSessionsRepo for message storage

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-13 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent*

### Debug Log References
_To be populated by dev agent*

### Completion Notes List
_To be populated by dev agent*

### File List
_To be populated by dev agent*

## QA Results
_To be populated by QA agent*
